{"cells":[{"metadata":{"id":"mqVupoblYAaj","outputId":"2cc053bd-039a-4073-f931-d1287fac8af4","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":1,"outputs":[{"output_type":"stream","text":"2.2.0\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"MvIOhOOICMTG"},"cell_type":"code","source":"import tensorflow.keras as tfk\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Flatten, BatchNormalization, MaxPool2D, GlobalAveragePooling2D, Dense, Dropout, Activation\nimport pandas as pd\nimport numpy as np","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"id":"j_Wd6MqFCMTO"},"cell_type":"code","source":"data_dir = \"../input/vietai-c6-assignment3-extracted-dataset/train.csv\"\nsub_dir = \"../input/vietai-c6-assignment3-extracted-dataset/sample_submission.csv\"\ntrain_df = pd.read_csv(data_dir)\nsub_df = pd.read_csv(sub_dir)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"id":"69COWsDICMTS","outputId":"8ede7305-5c14-4e3b-d948-6624fa6515ab"},"cell_type":"code","source":"train_df.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"      image  label\n0     0.jpg      0\n1     1.jpg      3\n2    10.jpg      2\n3   100.jpg      0\n4  1000.jpg      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000.jpg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"n3EHv2GNCMTZ","outputId":"02048819-3781-4508-fc7e-395ea852b6db"},"cell_type":"code","source":"sub_df.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"       image  label\n0  10010.jpg      0\n1  10011.jpg      0\n2  10028.jpg      0\n3  10034.jpg      0\n4  10056.jpg      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10010.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10011.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10028.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10034.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10056.jpg</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"7corOiJCCMTe"},"cell_type":"code","source":"classes = [\"book\", \"can\", \"cardboard\", \"glass_bottle\", \"pen\", \"plastic_bottle\"]\ntrain_y = train_y = train_df.label\nnum_classes = len(np.unique(train_y))\ny_ohe = tf.keras.utils.to_categorical(train_y, num_classes=num_classes)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"id":"L4KQsPdKCMTh","outputId":"b0b4af92-c04e-46fb-fd03-6b57e4646d00"},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nsize = 224\nbatch_size=32\ntrain_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2, horizontal_flip=True, vertical_flip=True)\ntrain_gen = train_data_gen.flow_from_directory(\"../input/vietai-c6-assignment3-extracted-dataset/train\", batch_size=batch_size,\n                                              target_size=(size, size), subset=\"training\")\n\nvalid_gen = train_data_gen.flow_from_directory(\"../input/vietai-c6-assignment3-extracted-dataset/train\", batch_size=batch_size,\n                                          target_size=(size, size), subset=\"validation\")","execution_count":7,"outputs":[{"output_type":"stream","text":"Found 25251 images belonging to 6 classes.\nFound 6308 images belonging to 6 classes.\n","name":"stdout"}]},{"metadata":{"id":"p_JBF6pyVkUR","trusted":true},"cell_type":"code","source":"def create_model():\n    model = tfk.Sequential()\n    model.add(Conv2D(144, (3, 3), strides=(1, 1), padding=\"valid\", input_shape=(224, 224, 3)))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPool2D())\n    model.add(Conv2D(94, (3, 3), strides=(1, 1), padding=\"valid\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPool2D())\n    model.add(Conv2D(144, (3, 3), strides=(1, 1), padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPool2D())\n    model.add(Conv2D(94, (3, 3), strides=(1, 1), padding=\"valid\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPool2D())\n    model.add(Conv2D(42, (3, 3), strides=(1, 1), padding=\"valid\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPool2D())\n    model.add(GlobalAveragePooling2D())\n    model.add(Flatten())\n    model.add(Dropout(0.21))\n    model.add(Dense(256, activation=\"tanh\", kernel_regularizer=tf.keras.regularizers.l2()))\n    model.add(Dropout(0.21))\n    model.add(Dense(256, activation=\"tanh\", kernel_regularizer=tf.keras.regularizers.l2()))\n    model.add(Dropout(0.21))\n    model.add(Dense(num_classes, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"CABYt5oPVoF1","trusted":true},"cell_type":"code","source":"def create_pretrained_model():\n    model = tfk.Sequential()\n    pretrained_net = tfk.applications.InceptionResNetV2(\n        include_top=False,\n        input_shape=(224, 224, 3),\n        pooling=\"avg\"\n    )\n    model.add(pretrained_net)\n    model.add(Dropout(0.42))\n    model.add(Dense(256, activation=\"tanh\", kernel_regularizer=tf.keras.regularizers.l2()))\n    model.add(Dropout(0.42))\n    model.add(Dense(128, activation=\"tanh\", kernel_regularizer=tf.keras.regularizers.l2()))\n    model.add(Dropout(0.42))\n    model.add(Dense(num_classes, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    print(model.summary())\n    return model","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"id":"WwWNxAxpCMTl"},"cell_type":"code","source":"class myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if logs.get(\"val_accuracy\") is not None:\n            if(logs.get('val_accuracy') > 0.99):\n                print(\"\\nReached 99% val_accuracy so cancelling training!\")\n                self.model.stop_training = True\nmcb = myCallback()","execution_count":9,"outputs":[]},{"metadata":{"id":"m6JONgdqgPpO","outputId":"cef55481-0c2d-4a08-9e90-98b47d39cedb","trusted":true},"cell_type":"code","source":"epochs = 150\nuse_efficientnet = True\nif not use_efficientnet:\n    model = create_model()\nelse:\n    model = create_pretrained_model()\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005*8\nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n\ndef lrfn(epoch):\n    if epoch < rampup_epochs:\n        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n    elif epoch < rampup_epochs + sustain_epochs:\n        return max_lr\n    else:\n        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n\nmcp = tf.keras.callbacks.ModelCheckpoint(\"my_model.h5\", monitor=\"val_accuracy\",\n                        save_best_only=True, save_weights_only=True, period=3)\nval_acc_earlyStop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n                                                         patience = epochs//15, restore_best_weights = True)\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n\n","execution_count":10,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n219062272/219055592 [==============================] - 2s 0us/step\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninception_resnet_v2 (Model)  (None, 1536)              54336736  \n_________________________________________________________________\ndropout (Dropout)            (None, 1536)              0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               393472    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 6)                 774       \n=================================================================\nTotal params: 54,763,878\nTrainable params: 54,703,334\nNon-trainable params: 60,544\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(\"../input/pretrained-inceptionres/my_model (1).h5\")","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(valid_gen)","execution_count":29,"outputs":[{"output_type":"stream","text":"198/198 [==============================] - 23s 116ms/step - loss: 0.1618 - accuracy: 0.9662\n","name":"stdout"},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"[0.16177572309970856, 0.9662333726882935]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2, horizontal_flip=True, vertical_flip=True)\ntrain_gen = train_data_gen.flow_from_directory(\"../input/vietai-c6-assignment3-extracted-dataset/train\", batch_size=batch_size,\n                                              target_size=(size, size), subset=\"training\", seed=42)\n\nvalid_gen = train_data_gen.flow_from_directory(\"../input/vietai-c6-assignment3-extracted-dataset/train\", batch_size=batch_size,\n                                          target_size=(size, size), subset=\"validation\", seed=42)","execution_count":35,"outputs":[{"output_type":"stream","text":"Found 25251 images belonging to 6 classes.\nFound 6308 images belonging to 6 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_gen, validation_data=valid_gen, epochs=epochs,initial_epoch=45, callbacks=[mcp, mcb, val_acc_earlyStop, lr_callback])","execution_count":36,"outputs":[{"output_type":"stream","text":"\nEpoch 00046: LearningRateScheduler reducing learning rate to 1.0051839891835612e-05.\nEpoch 46/150\n790/790 [==============================] - 296s 375ms/step - loss: 0.0173 - accuracy: 0.9999 - val_loss: 0.1714 - val_accuracy: 0.9680 - lr: 1.0052e-05\n\nEpoch 00047: LearningRateScheduler reducing learning rate to 1.004147191346849e-05.\nEpoch 47/150\n790/790 [==============================] - 297s 376ms/step - loss: 0.0184 - accuracy: 0.9996 - val_loss: 0.1723 - val_accuracy: 0.9673 - lr: 1.0041e-05\n\nEpoch 00048: LearningRateScheduler reducing learning rate to 1.0033177530774792e-05.\nEpoch 48/150\n790/790 [==============================] - 297s 376ms/step - loss: 0.0168 - accuracy: 0.9999 - val_loss: 0.1838 - val_accuracy: 0.9650 - lr: 1.0033e-05\n\nEpoch 00049: LearningRateScheduler reducing learning rate to 1.0026542024619834e-05.\nEpoch 49/150\n790/790 [==============================] - 296s 375ms/step - loss: 0.0167 - accuracy: 0.9999 - val_loss: 0.1736 - val_accuracy: 0.9681 - lr: 1.0027e-05\n\nEpoch 00050: LearningRateScheduler reducing learning rate to 1.0021233619695867e-05.\nEpoch 50/150\n790/790 [==============================] - 296s 374ms/step - loss: 0.0169 - accuracy: 0.9998 - val_loss: 0.1633 - val_accuracy: 0.9705 - lr: 1.0021e-05\n\nEpoch 00051: LearningRateScheduler reducing learning rate to 1.0016986895756694e-05.\nEpoch 51/150\n790/790 [==============================] - 296s 374ms/step - loss: 0.0166 - accuracy: 0.9998 - val_loss: 0.1754 - val_accuracy: 0.9677 - lr: 1.0017e-05\n\nEpoch 00052: LearningRateScheduler reducing learning rate to 1.0013589516605356e-05.\nEpoch 52/150\n790/790 [==============================] - 295s 374ms/step - loss: 0.0164 - accuracy: 0.9999 - val_loss: 0.1813 - val_accuracy: 0.9680 - lr: 1.0014e-05\n\nEpoch 00053: LearningRateScheduler reducing learning rate to 1.0010871613284285e-05.\nEpoch 53/150\n790/790 [==============================] - 296s 374ms/step - loss: 0.0159 - accuracy: 0.9999 - val_loss: 0.1804 - val_accuracy: 0.9661 - lr: 1.0011e-05\n\nEpoch 00054: LearningRateScheduler reducing learning rate to 1.0008697290627427e-05.\nEpoch 54/150\n790/790 [==============================] - 297s 375ms/step - loss: 0.0165 - accuracy: 0.9998 - val_loss: 0.1629 - val_accuracy: 0.9680 - lr: 1.0009e-05\n\nEpoch 00055: LearningRateScheduler reducing learning rate to 1.0006957832501943e-05.\nEpoch 55/150\n790/790 [==============================] - 296s 374ms/step - loss: 0.0159 - accuracy: 0.9998 - val_loss: 0.1802 - val_accuracy: 0.9670 - lr: 1.0007e-05\n\nEpoch 00056: LearningRateScheduler reducing learning rate to 1.0005566266001554e-05.\nEpoch 56/150\n790/790 [==============================] - 295s 374ms/step - loss: 0.0157 - accuracy: 0.9998 - val_loss: 0.1772 - val_accuracy: 0.9688 - lr: 1.0006e-05\n\nEpoch 00057: LearningRateScheduler reducing learning rate to 1.0004453012801243e-05.\nEpoch 57/150\n790/790 [==============================] - 297s 375ms/step - loss: 0.0152 - accuracy: 0.9999 - val_loss: 0.1765 - val_accuracy: 0.9685 - lr: 1.0004e-05\n\nEpoch 00058: LearningRateScheduler reducing learning rate to 1.0003562410240995e-05.\nEpoch 58/150\n790/790 [==============================] - 295s 374ms/step - loss: 0.0154 - accuracy: 0.9997 - val_loss: 0.1740 - val_accuracy: 0.9689 - lr: 1.0004e-05\n\nEpoch 00059: LearningRateScheduler reducing learning rate to 1.0002849928192796e-05.\nEpoch 59/150\n790/790 [==============================] - 296s 375ms/step - loss: 0.0156 - accuracy: 0.9998 - val_loss: 0.1893 - val_accuracy: 0.9648 - lr: 1.0003e-05\n\nEpoch 00060: LearningRateScheduler reducing learning rate to 1.0002279942554237e-05.\nEpoch 60/150\n790/790 [==============================] - 297s 376ms/step - loss: 0.0155 - accuracy: 0.9998 - val_loss: 0.1843 - val_accuracy: 0.9669 - lr: 1.0002e-05\n","name":"stdout"},{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f6822f0c090>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"weight_model.h5\")\nmodel.save(\"full_model.h5\")","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(valid_gen)","execution_count":41,"outputs":[{"output_type":"stream","text":"198/198 [==============================] - 23s 115ms/step - loss: 0.1751 - accuracy: 0.9683\n","name":"stdout"},{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"[0.17513923346996307, 0.9682942032814026]"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"Hrgc1T-LCMT1"},"cell_type":"code","source":"test_data_gen = ImageDataGenerator(rescale=1.0/255)\ntest_generator = test_data_gen.flow_from_directory(\"../input/vietai-c6-assignment3-extracted-dataset/test\",class_mode=None, target_size=(size, size), shuffle=False)\ntest_generator.reset()\npred = model.predict(test_generator)\n\n# pred là một ma trận xác suất của ảnh trên các lớp.\n# Ta lấy lớp có xác suất cao nhất trên từng ảnh bằng hàm argmax\npred_labels = np.argmax(pred, axis=1)\nsub_df['label'] = pred_labels\nsub_df.head(20)","execution_count":42,"outputs":[{"output_type":"stream","text":"Found 3837 images belonging to 1 classes.\n","name":"stdout"},{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"        image  label\n0   10010.jpg      2\n1   10011.jpg      1\n2   10028.jpg      2\n3   10034.jpg      3\n4   10056.jpg      5\n5   10081.jpg      2\n6   10084.jpg      1\n7   10091.jpg      3\n8     101.jpg      2\n9    1010.jpg      5\n10  10107.jpg      0\n11  10115.jpg      1\n12  10132.jpg      0\n13  10133.jpg      3\n14  10137.jpg      4\n15  10138.jpg      0\n16  10167.jpg      5\n17   1017.jpg      0\n18  10204.jpg      3\n19  10208.jpg      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10010.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10011.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10028.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10034.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10056.jpg</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10081.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10084.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10091.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>101.jpg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1010.jpg</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10107.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10115.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>10132.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>10133.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>10137.jpg</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>10138.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>10167.jpg</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1017.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>10204.jpg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>10208.jpg</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"riWkqzkACMT7"},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":34,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}