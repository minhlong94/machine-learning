{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tutorial link: https://phamdinhkhanh.github.io/2020/06/04/PhoBERT_Fairseq.html#8-b%C3%A0i-to%C3%A1n-classification\n\n## Note: I tried to use PhoBERT for sentiment analysis, but seemed like it was underfit because of small dataset I used."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%%bash\npip3 install vncorenlp\nwget https://public.vinai.io/PhoBERT_large_transformers.tar.gz\ntar -xzvf PhoBERT_large_transformers.tar.gz\nmkdir -p vncorenlp/models/wordsegmenter\nwget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\nwget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\nwget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\nmv VnCoreNLP-1.1.1.jar vncorenlp/ \nmv vi-vocab vncorenlp/models/wordsegmenter/\nmv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\npip3 install fairseq\npip3 install fastbpe\npip3 install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n!tar -xzvf PhoBERT_base_fairseq.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport time\nimport datetime\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport keras.backend as K\n\nimport warnings\nfrom gensim.models import FastText\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.remove(\"./PhoBERT_base_fairseq.tar.gz\")\nos.remove(\"./PhoBERT_large_transformers.tar.gz\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.display.max_colwidth=1000\ntrain_df = pd.read_csv(\"../input/vietai-dataset/assignment4-data/Assignment4/train.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(\"../input/vietai-dataset/assignment4-data/Assignment4/test.csv\")\nprint('Number of test samples in total:', len(test_df))\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n# re = regular expressions\nstrip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n\ndef clean_sentences(string):\n    string = string.lower().replace(\"<br />\", \" \")\n    return re.sub(strip_special_chars, \"\", string.lower())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_sentences =[clean_sentences(row) for row in train_df[\"text\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cleaned_sentences[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"text\"] = cleaned_sentences\ntrain_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped_train = train_df.drop(columns=\"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropped_train.to_csv(\"processed_train.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the model in fairseq\nfrom fairseq.models.roberta import RobertaModel\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\n\nphoBERT_cls = RobertaModel.from_pretrained('./PhoBERT_base_fairseq', checkpoint_file='model.pt')\nphoBERT_cls.eval()  # disable dropout (or leave in train mode to finetune\n\n# Load BPE\nclass BPE():\n    bpe_codes = './PhoBERT_base_fairseq/bpe.codes'\n\nargs = BPE()\nphoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n\n# Add header cho classification với số lượng classes = 10\nphoBERT_cls.register_classification_head('new_task', num_classes=10)\ntokens = 'Học_sinh được nghỉ học bắt đầu từ tháng 3 do ảnh hưởng của dịch covid-19'\ntoken_idxs = phoBERT_cls.encode(tokens)\nlogprobs = phoBERT_cls.predict('new_task', token_idxs)  # tensor([[-1.1050, -1.0672, -1.1245]], grad_fn=<LogSoftmaxBackward>)\nlogprobs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef evaluate(logits, targets):\n    \"\"\"\n    Đánh giá model sử dụng accuracy và f1 scores.\n    Args:\n        logits (B,C): torch.LongTensor. giá trị predicted logit cho class output.\n        targets (B): torch.LongTensor. actual target indices.\n    Returns:\n        acc (float): the accuracy score\n        f1 (float): the f1 score\n    \"\"\"\n    # Tính accuracy score và f1_score\n    logits = logits.detach().cpu().numpy()    \n    y_pred = np.argmax(logits, axis = 1)\n    targets = targets.detach().cpu().numpy()\n    f1 = f1_score(targets, y_pred, average='weighted')\n    acc = accuracy_score(targets, y_pred)\n    return acc, f1\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nlogits = torch.tensor([[0.1, 0.2, 0.7],\n                       [0.4, 0.1, 0.5],\n                       [0.1, 0.2, 0.7]]).to(device)\ntargets = torch.tensor([1, 2, 2]).to(device)\nevaluate(logits, targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def validate(valid_loader, model, device):\n    model.eval()\n    accs = []\n    f1s = []\n    with torch.no_grad():\n        for x_batch, y_batch in valid_loader:\n            x_batch = x_batch.to(device)\n            y_batch = y_batch.to(device)\n            outputs = model.predict('new_task', x_batch)\n            logits = torch.exp(outputs)\n            acc, f1 = evaluate(logits, y_batch)\n            accs.append(acc)\n            f1s.append(f1)\n    \n    mean_acc = np.mean(accs)\n    mean_f1 = np.mean(f1s)\n    return mean_acc, mean_f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def trainOnEpoch(train_loader, model, optimizer, epoch, num_epochs, criteria, device, log_aggr = 100):\n    model.train()\n    sum_epoch_loss = 0\n    sum_acc = 0\n    sum_f1 = 0\n    start = time.time()\n    for i, (x_batch, y_batch) in enumerate(train_loader):\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        optimizer.zero_grad()\n        y_pred = model.predict('new_task', x_batch)\n        logits = torch.exp(y_pred)\n        acc, f1 = evaluate(logits, y_batch)\n        loss = criteria(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        loss_val = loss.item()\n        sum_epoch_loss += loss_val\n        sum_acc += acc\n        sum_f1 += f1\n        iter_num = epoch * len(train_loader) + i + 1\n\n        if i % log_aggr == 0:\n            print('[TRAIN] epoch %d/%d  observation %d/%d batch loss: %.4f (avg %.4f),  avg acc: %.4f, avg f1: %.4f, (%.2f im/s)'\n                % (epoch + 1, num_epochs, i, len(train_loader), loss_val, sum_epoch_loss / (i + 1),  sum_acc/(i+1), sum_f1/(i+1),\n                  len(x_batch) / (time.time() - start)))\n        start = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\nmax_sequence_length = 256\ndef convert_lines(lines, vocab, bpe):\n    '''\n    lines: list các văn bản input\n    vocab: từ điển dùng để encoding subwords\n    bpe: \n    '''\n    # Khởi tạo ma trận output\n    outputs = np.zeros((len(lines), max_sequence_length), dtype=np.int32) # --> shape (number_lines, max_seq_len)\n    # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n    cls_id = 0\n    eos_id = 2\n    pad_id = 1\n\n    for idx, row in tqdm(enumerate(lines), total=len(lines)): \n        # Mã hóa subwords theo byte pair encoding(bpe)\n        subwords = bpe.encode('<s> '+ row +' </s>')\n        input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n        # Truncate input nếu độ dài vượt quá max_seq_len\n        if len(input_ids) > max_sequence_length: \n            input_ids = input_ids[:max_sequence_length] \n            input_ids[-1] = eos_id\n        else:\n          # Padding nếu độ dài câu chưa bằng max_seq_len\n            input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n\n        outputs[idx,:] = np.array(input_ids)\n    return outputs\n\n# Load the dictionary  \nvocab = Dictionary()\nvocab.add_from_file(\"./PhoBERT_large_transformers/dict.txt\")\n\n\n# Test encode lines\nlines = ['Học_sinh được nghỉ học bắt dầu từ tháng 3 để tránh dịch covid-19', 'số lượng ca nhiễm bệnh đã giảm bắt đầu từ tháng 5 nhờ biện pháp mạnh tay']\n[x1, x2] = convert_lines(lines, vocab, phoBERT_cls.bpe)\nprint('x1 tensor encode: {}, shape: {}'.format(x1[:10], x1.size))\nprint('x1 tensor decode: ', phoBERT_cls.decode(torch.tensor(x1))[:103])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dropped_train[\"text\"]\ny = dropped_train[\"class\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = convert_lines(X, vocab, phoBERT_cls.bpe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\nlb.fit(y)\ny = lb.fit_transform(y)\nprint(lb.classes_)\nprint('Top classes indices: ', y[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\ndef _save_pkl(path, obj):\n    with open(path, 'wb') as f:\n        pickle.dump(obj, f)\n        \n_save_pkl('X1.pkl', X)\n_save_pkl('y1.pkl', y)\n_save_pkl('labelEncoder1.pkl', lb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport time\nimport random\nimport argparse\nimport pickle\nimport numpy as np\nfrom tqdm import tqdm\nfrom os.path import join\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.autograd import Variable\nfrom torch.backends import cudnn\nfrom sklearn.model_selection import StratifiedKFold\n\n# Load the model in fairseq\nfrom fairseq.models.roberta import RobertaModel\nfrom fairseq.data.encoders.fastbpe import fastBPE\nfrom fairseq.data import Dictionary\nfrom transformers.modeling_utils import * \nfrom transformers import *\n\n# Khởi tạo argument\nEPOCHS = 20\nBATCH_SIZE = 6\nACCUMULATION_STEPS = 5\nFOLD = 4\nLR = 0.0001\nLR_DC_STEP = 80 \nLR_DC = 0.1\nCUR_DIR = os.path.dirname(os.getcwd())\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nFOLD = 4\nCKPT_PATH2 = 'model_ckpt2'\n\nif not os.path.exists(CKPT_PATH2):\n    os.mkdir(CKPT_PATH2)\n\n# Khởi tạo DataLoader\nsplits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=123).split(X, y))\n\nfor fold, (train_idx, val_idx) in enumerate(splits):\n    best_score = 0\n    if fold != FOLD:\n        continue\n    print(\"Training for fold {}\".format(fold))\n    \n    # Create dataset\n    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X[train_idx],dtype=torch.long), torch.tensor(y[train_idx],dtype=torch.long))\n    valid_dataset = torch.utils.data.TensorDataset(torch.tensor(X[val_idx],dtype=torch.long), torch.tensor(y[val_idx],dtype=torch.long))\n\n    # Create DataLoader\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    # Khởi tạo model:\n    MODEL_LAST_CKPT = os.path.join(CKPT_PATH2, 'latest_checkpoint.pth.tar')\n    if os.path.exists(MODEL_LAST_CKPT):\n        print('Load checkpoint model!') \n        phoBERT_cls = torch.load(MODEL_LAST_CKPT)\n    else:\n        print('Load model pretrained!')\n      # Load the model in fairseq\n        from fairseq.models.roberta import RobertaModel\n        from fairseq.data.encoders.fastbpe import fastBPE\n        from fairseq.data import Dictionary\n\n        phoBERT_cls = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n        phoBERT_cls.eval()  # disable dropout (or leave in train mode to finetune\n\n      # # Load BPE\n      # class BPE():\n      #   bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n\n      # args = BPE()\n      # phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n\n      # Add header cho classification với số lượng classes = 10\n        phoBERT_cls.register_classification_head('new_task', num_classes=10)\n      \n    ## Load BPE\n    print('Load BPE')\n    class BPE():\n        bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n\n    args = BPE()\n    phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n    phoBERT_cls.to(DEVICE)\n\n    # Khởi tạo optimizer và scheduler, criteria\n    print('Init Optimizer, scheduler, criteria')\n    param_optimizer = list(phoBERT_cls.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n\n    num_train_optimization_steps = int(EPOCHS*len(train_dataset)/BATCH_SIZE/ACCUMULATION_STEPS)\n    optimizer = AdamW(optimizer_grouped_parameters, lr=LR, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # scheduler với linear warmup\n    scheduler0 = get_constant_schedule(optimizer)  # scheduler với hằng số\n    # optimizer = optim.Adam(phoBERT_cls.parameters(), LR)\n    criteria = nn.NLLLoss()\n    # scheduler = StepLR(optimizer, step_size = LR_DC_STEP, gamma = LR_DC)\n    avg_loss = 0.\n    avg_accuracy = 0.\n    frozen = True\n    for epoch in tqdm(range(EPOCHS)):\n        # warm up tại epoch đầu tiên, sau epoch đầu sẽ phá băng các layers\n        if epoch > 0 and frozen:\n            for child in phoBERT_cls.children():\n                for param in child.parameters():\n                    param.requires_grad = True\n            frozen = False\n            del scheduler0\n            torch.cuda.empty_cache()\n        # Train model on EPOCH\n        print('Epoch: ', epoch)\n        trainOnEpoch(train_loader=train_loader, model=phoBERT_cls, optimizer=optimizer, epoch=epoch, num_epochs=EPOCHS, criteria=criteria, device=DEVICE, log_aggr=100)\n        # scheduler.step(epoch = epoch)\n        # Phá băng layers sau epoch đầu tiên\n        if not frozen:\n            scheduler.step()\n        else:\n            scheduler0.step()\n        optimizer.zero_grad()\n        # Validate on validation set\n        acc, f1 = validate(valid_loader, phoBERT_cls, device=DEVICE)\n        print('Epoch {} validation: acc: {:.4f}, f1: {:.4f} \\n'.format(epoch, acc, f1))\n\n        # Store best model checkpoint\n        ckpt_dict = {\n            'epoch': epoch + 1,\n            'state_dict': phoBERT_cls.state_dict(),\n            'optimizer': optimizer.state_dict()\n        }\n        # Save model checkpoint into 'latest_checkpoint.pth.tar'\n        torch.save(ckpt_dict, MODEL_LAST_CKPT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}