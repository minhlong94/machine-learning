{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN1_Text_Summarization_Assignment",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhMXbocR21cX",
        "colab_type": "text"
      },
      "source": [
        "# Assignment: use RNNs to do Text classification, Text generation, Text summarization and Machine translation\n",
        "\n",
        "Author: Long M. Luu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6xLXSwwaI1Y",
        "colab_type": "text"
      },
      "source": [
        "## Part 1: text summarization\n",
        "The News20Group dataset will be downloaded. The job is to create a model that can summarize it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaXSzALR2gS2",
        "colab_type": "text"
      },
      "source": [
        "## Download GloVe word vectors: LOAD THIS PART\n",
        "Other options are:  \n",
        "glove.6B.zip: from Wikipedia + Gigaword, 6B tokens, 400K vocab, uncased, 50d, 100d, 200d and 300d vectors, 822MB download  \n",
        "glove.42B.300d.zip: from Common Crawl, 42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download  \n",
        "glove.840B.300d.zip: from Common Crawl, 840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download  \n",
        "glove.twitter.27B.zip: from Twitter, 2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOY1Oeppw1VW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "%%bash\n",
        "wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFZsRHMatWNU",
        "colab_type": "text"
      },
      "source": [
        "Might took a while to download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZSzuFvX4N5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AAIxyBBaDmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract file to load word embeddings\n",
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \"/content/glove.6B.100d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3JHQHmXiGVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print an example of word embedding\n",
        "print(list(embeddings_index.items())[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qu0zXGE72hi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print an example of word embedding\n",
        "print(list(embeddings_index.items())[42])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE7NRs-c4QAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Newsground20 dataset\n",
        "# It is News from BBC, categoried into 20 categories\n",
        "data_path = keras.utils.get_file(\n",
        "    \"news20.tar.gz\",\n",
        "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
        "    extract=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDb5v6U17ivU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Create directory and move corresponding label to that directory\n",
        "os.listdir(pathlib.Path(data_path).parent)\n",
        "data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n",
        "dirnames = os.listdir(data_dir)\n",
        "print(\"Number of directories:\", len(dirnames))\n",
        "print(\"Directory names:\", dirnames)\n",
        "\n",
        "fnames = os.listdir(data_dir / \"comp.graphics\")\n",
        "print(\"Number of files in comp.graphics:\", len(fnames))\n",
        "print(\"Some example filenames:\", fnames[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5mR2O9O76ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read an example\n",
        "print(open(data_dir / \"comp.graphics\" / \"38987\").read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCmJ88di2F5Y",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQd3XgEXCQ23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete headers, get class names and indicies\n",
        "samples = []\n",
        "labels = []\n",
        "class_names = []\n",
        "class_index = 0\n",
        "for dirname in sorted(os.listdir(data_dir)):\n",
        "    class_names.append(dirname)\n",
        "    dirpath = data_dir / dirname\n",
        "    fnames = os.listdir(dirpath)\n",
        "    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n",
        "    for fname in fnames:\n",
        "        fpath = dirpath / fname\n",
        "        f = open(fpath, encoding=\"latin-1\")\n",
        "        content = f.read()\n",
        "        lines = content.split(\"\\n\")\n",
        "        lines = lines[10:]\n",
        "        content = \"\\n\".join(lines)\n",
        "        samples.append(content)\n",
        "        labels.append(class_index)\n",
        "    class_index += 1\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Number of samples:\", len(samples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQr4j_XMDyrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read a deleted header sample\n",
        "print(samples[42], labels[42], class_names[labels[42]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7bOnNqMSC_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "TODO 1: Preprocess data\n",
        "For each element in \"samples\", call \"preprocess\" function for that element\n",
        "Append all results in a list called processed_samples\n",
        "\"\"\"\n",
        "\n",
        "# Preprocess data function\n",
        "def preprocess(data):\n",
        "    '''\n",
        "    Preprocess data: all characters are converted into lowercase and special characters are removed\n",
        "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
        "\n",
        "    Arguments:\n",
        "        data: a string\n",
        "    Returns:\n",
        "        text: preprocessed version of \"data\"\n",
        "    '''\n",
        "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "    def clean_special_chars(text, punct):\n",
        "        for p in punct:\n",
        "            text = text.replace(p, ' ')\n",
        "        return text\n",
        "\n",
        "    return clean_special_chars(data, punct)\n",
        "\n",
        "### START CODE HERE\n",
        "processed_samples = None\n",
        "### END CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqtihOYYUEVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unprocessed and processed data\n",
        "print(samples[0], end=\"\\n------------------\\n\")\n",
        "print(processed_samples[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhjrv3b5lNIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data intro valid and train\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_samples, val_samples, train_labels, val_labels = train_test_split(processed_samples, labels,\n",
        "                                                                        test_size=0.2, random_state=42, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgsyCw6eVFJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate total number of samples, total number of words in sample, and average number of words in each sample\n",
        "num_words = [len(x.split()) for x in processed_samples]\n",
        "print('The total number of samples is', len(processed_samples))\n",
        "print('The total number of words in the files is', sum(num_words))\n",
        "print('The average number of words in the files is', sum(num_words)/len(num_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOV24jWpVaaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the histogram\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.hist(num_words, bins=\"auto\")\n",
        "plt.xlabel('Num of words in sentences')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ex7QnQ_Xqdu",
        "colab_type": "text"
      },
      "source": [
        "Given the Histogram plot, it is reasonable to choose max sentence length = 600"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuL7b93GFMGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "TODO 2: Create vocabulary index with TextVectorization\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization\n",
        "Let: max_tokens = vocab_size, output_sequence_length = max_sentence_length\n",
        "\n",
        "Then, create text_dataset by calling tf.data.Dataset.from_tensor_slices\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\n",
        "Then call function batch() of \"text_dataset\", and pass in batch_size\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "vocab_size = 20000 # Only take top 20k words of the vocab\n",
        "max_sentence_length = 400 # Max input length, exceeded words will be padded\n",
        "batch_size = 64 \n",
        "\n",
        "### START CODE HERE\n",
        "vectorizer = None\n",
        "text_dataset = None\n",
        "### END CODE HERE\n",
        "\n",
        "vectorizer.adapt(text_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU46FzxTZViC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer.get_vocabulary()[:10] # Get 10 examples of vocabulary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4FL7OxkFuPo",
        "colab_type": "text"
      },
      "source": [
        "Expected output: `['', '[UNK]', 'the', 'to', 'of', 'a', 'and', 'i', 'in', 'is']`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HcjbS_GQ33D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sample sentence that is vectorzied\n",
        "sample_output = vectorizer(np.array([[\"I am learning text vectorization\"]]))\n",
        "sample_output.numpy()[0, :10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSqmpxk2FySK",
        "colab_type": "text"
      },
      "source": [
        "Expected output: `array([   7,  115, 2888,  660,    1,    0,    0,    0,    0,    0])`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfpRkT4mZxoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "print(len(voc))\n",
        "word_index = dict(zip(voc, range(2, len(voc))))\n",
        "print(list(word_index.items())[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfBR_oFaF3Aj",
        "colab_type": "text"
      },
      "source": [
        "Expected output: `20000\n",
        "[('', 2), ('[UNK]', 3), ('the', 4), ('to', 5), ('of', 6), ('a', 7), ('and', 8), ('i', 9), ('in', 10), ('is', 11)]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lEhPqvJeYJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWrQl8fG1msu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=50\n",
        "model_cp = tf.keras.callbacks.ModelCheckpoint(\"model_cp\", monitor=\"val_loss\", save_format=\"tf\", save_best_only=True)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n",
        "                                              patience=epochs//10, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWI_4X9Z17W-",
        "colab_type": "text"
      },
      "source": [
        "## Create a simple RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWEXs1gri0GX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "TODO 3: create a model\n",
        "The layers of Sequential are as follows:\n",
        "\n",
        "Input, has shape (1, ), and dtype is tf.string: https://www.tensorflow.org/api_docs/python/tf/keras/Input?hl=en\n",
        "\"vectorizer\" variable (TextVectorization layer defined above)\n",
        "Embedding layer: input_dim is \"num_tokens\", output dim is \"embedding_dim\", embeddings_initializer is Contstant(embedding_matrix), set trainable=False\n",
        "SimpleRNN, 100 units, return_sequences is True: https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN?hl=en\n",
        "SimpleRNN, 50 units\n",
        "Dense, 128 units, activation tanh\n",
        "Dense, 64 units, activation tanh\n",
        "Dense, len(class_names), activation softmax\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "# Sample model 1: normal RNN\n",
        "\n",
        "def create_simple_rnn_model():\n",
        "    model = None\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "simple_rnn_model = create_simple_rnn_model()\n",
        "simple_rnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTxyuy2ClPD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_1 = simple_rnn_model.fit(train_samples, train_labels, \n",
        "                                 batch_size=128, epochs=5, validation_data=(val_samples, val_labels), \n",
        "                                 callbacks=[model_cp, early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4-cTypN2Plh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot accuracy vs epoch\n",
        "plt.plot(history_1.history['accuracy'])\n",
        "plt.plot(history_1.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj6hvpLi2Ml9",
        "colab_type": "text"
      },
      "source": [
        "## Create a Bidirectional with GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG4fhOhS5tVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "TODO 4: create a bidirectional model\n",
        "The layers of Sequential are as follows:\n",
        "\n",
        "Input, has shape (1, ), and dtype is tf.string: https://www.tensorflow.org/api_docs/python/tf/keras/Input?hl=en\n",
        "\"vectorizer\" variable (TextVectorization layer defined above)\n",
        "Embedding layer: input_dim is \"num_tokens\", output dim is \"embedding_dim\", embeddings_initializer is Contstant(embedding_matrix), set trainable=False\n",
        "Bidirectional GRU, 128 units, return_sequences is True: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n",
        "Bidirectional GRU, 64 units\n",
        "Dense, 64 units, activation tanh\n",
        "Dense, len(class_names), activation softmax\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.layers import Dense, GRU, Bidirectional\n",
        "\n",
        "# Sample model 2: Bidirectional with GRU\n",
        "def create_bidi_gru_model():\n",
        "    model = None\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "bidi_gru_model = create_bidi_gru_model()\n",
        "bidi_gru_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_2h0p2mGf7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cp2 = tf.keras.callbacks.ModelCheckpoint(\"model_cp\", monitor=\"val_loss\", save_format=\"tf\", save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAaB4NPZ7sVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_2 = bidi_gru_model.fit(train_samples, train_labels, \n",
        "                               batch_size=128, epochs=10, validation_data=(val_samples, val_labels),\n",
        "                               callbacks=[model_cp2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO8QeHfp2Wt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot accuracy vs epoch\n",
        "plt.plot(history_2.history['accuracy'])\n",
        "plt.plot(history_2.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx2isbhGIi3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict\n",
        "class_names[np.argmax(bidi_gru_model.predict(np.array([[\"The PC performance is very bad. You should buy a laptop instead.\"]])))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otMX4rk57yxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your custom model\n",
        "\n",
        "def create_custom_model():\n",
        "    model = None\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "custom_model = create_custom_model()\n",
        "custom_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YH4CzIbGiZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cp3 = tf.keras.callbacks.ModelCheckpoint(\"model_cp\", \n",
        "                                               monitor=\"val_loss\", save_format=\"tf\", save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwPE5ySh2cgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_3 = custom_model.fit(train_samples, train_labels, \n",
        "                               batch_size=128, epochs=10, validation_data=(val_samples, val_labels),\n",
        "                             callbacks=[model_cp3, early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}