# Papers

Here I list all the papers that I read, which in the future I might need to read again.

## Computer Vision

### [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)

## Natural Language Processing

### [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)

### [Attention Is All You Need](https://arxiv.org/abs/1706.03762)


### [PhoBERT: Pre-trained language models for Vietnamese](https://arxiv.org/abs/2003.00744)


### [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
## Others
### [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)

## Others
These are the other papers that I read or know, but I do not carefully study them.
### [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
### [Quantum supremacy using a programmable superconducting processor](https://www.nature.com/articles/s41586-019-1666-5)
### [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
### [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
### [Algorithms for hyper-parameter optimization](http://papers.nips.cc/paper/4443-algorithms-for-hyper)
### [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661.pdf)
### [Quasi-hyperbolic momentum and Adam for deep learning](https://arxiv.org/abs/1810.06801)
### [Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks](https://arxiv.org/abs/1602.07868)
### [On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265)
### [NeuMiss networks: differential programming for supervised learning with missing values](https://arxiv.org/abs/2007.01627)
### [Untangling tradeoffs between recurrence and self-attention in neural networks](https://arxiv.org/abs/2006.09471)
### [ArcFace: Additive Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/1801.07698)
### [Train longer, generalize better: closing the generalization gap in large batch training of neural networks](https://arxiv.org/abs/1705.08741)
### [DeepInsight: A methodology to transform a non-image data to an image for convolution neural network architecture](https://www.nature.com/articles/s41598-019-47765-6)
### [Old Photo Restoration via Deep Latent Space Translation](https://arxiv.org/abs/2009.07047)

