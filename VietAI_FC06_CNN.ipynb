{"cells":[{"metadata":{},"cell_type":"markdown","source":"Competition page: https://www.kaggle.com/c/vietai-foundation-course-cnn-assignment  \nOriginal dataset: https://www.kaggle.com/c/vietai-foundation-course-cnn-assignment/data  \nExtracted dataset (to be used in this notebook): https://www.kaggle.com/aeryss/vietai-c6-assignment3-extracted-dataset  "},{"metadata":{"id":"mqVupoblYAaj","outputId":"2cc053bd-039a-4073-f931-d1287fac8af4","trusted":false},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"MvIOhOOICMTG","trusted":false},"cell_type":"code","source":"import tensorflow.keras as tfk\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Flatten, BatchNormalization, MaxPool2D, GlobalAveragePooling2D, Dense, Dropout, Activation\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"id":"j_Wd6MqFCMTO","trusted":false},"cell_type":"code","source":"# # Data is seperated using this piece of code:\n\n# import shutil\n# train_dir = \"Assignment3Data/train\"\n# test_dir = \"Assignment3Data/test\"\n\n\n# try:\n#     shutil.rmtree(train_dir)\n# except Exception:\n#     pass\n# try:\n#     shutil.rmtree(test_dir)\n# except Exception:\n#     pass\n    \n# os.makedirs(train_dir)\n# os.makedirs(test_dir)\n\n# train_y = train_df.label\n# num_classes = len(np.unique(train_y))\n# y_ohe = tf.keras.utils.to_categorical(train_y, num_classes=num_classes)\n\n# classes = [\"book\", \"can\", \"cardboard\", \"glass_bottle\", \"pen\", \"plastic_bottle\"]\n# for eachClass in classes:\n#     os.makedirs(train_dir + \"/\" + eachClass)\n\n# train_file_path = [os.path.join(\"../input/vietai-foundation-course-cnn-assignment/Assignment3Data/images\", file) for file in train_df.image]\n# test_file_path = [os.path.join(\"../input/vietai-foundation-course-cnn-assignment/Assignment3Data/images\", file) for file in test_df.image]\n# # for train_image in train_file_path:\n# #     shutil.copy(train_image, train_dir)\n\n# for i in range(len(train_file_path)):\n#     shutil.copy(train_file_path[i], train_dir + \"/\" + classes[train_df.loc[i, \"label\"]])\n# for test_image in test_file_path:\n#     shutil.copy(test_image, test_dir)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"../input/vietai-c6-assignment3-extracted-dataset/train.csv\"\nsub_dir = \"../input/vietai-c6-assignment3-extracted-dataset/sample_submission.csv\"\ntrain_df = pd.read_csv(data_dir)\nsub_df = pd.read_csv(sub_dir)","execution_count":null,"outputs":[]},{"metadata":{"id":"69COWsDICMTS","outputId":"8ede7305-5c14-4e3b-d948-6624fa6515ab","trusted":false},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"n3EHv2GNCMTZ","outputId":"02048819-3781-4508-fc7e-395ea852b6db","trusted":false},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"7corOiJCCMTe","trusted":false},"cell_type":"code","source":"classes = [\"book\", \"can\", \"cardboard\", \"glass_bottle\", \"pen\", \"plastic_bottle\"]\ntrain_y = train_y = train_df.label\nnum_classes = len(np.unique(train_y))\ny_ohe = tf.keras.utils.to_categorical(train_y, num_classes=num_classes)","execution_count":null,"outputs":[]},{"metadata":{"id":"L4KQsPdKCMTh","outputId":"b0b4af92-c04e-46fb-fd03-6b57e4646d00","trusted":false},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nsize = 224\nbatch_size=32\n# Use ImageDataGenerator, because converting into NumPy is costly\ntrain_data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # Intended to aug images, but it is costly\ntrain_gen = train_data_gen.flow_from_directory(\"../input/vietai-c6-assignment3-extracted-dataset/train\", batch_size=batch_size,\n                                              target_size=(size, size), subset=\"training\")\nvalid_gen = train_data_gen.flow_from_directory(\"../input/vietai-c6-assignment3-extracted-dataset/train\", batch_size=batch_size,\n                                              target_size=(size, size), subset=\"validation\")","execution_count":null,"outputs":[]},{"metadata":{"id":"FECQnh8iZniz","trusted":false},"cell_type":"code","source":"# # Tried using TPU but always fail, don't know why\n\n\n# try:\n#   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n#   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n# except ValueError:\n#   raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":null,"outputs":[]},{"metadata":{"id":"p_JBF6pyVkUR","trusted":false},"cell_type":"code","source":"# Basic model\ndef create_model():\n    model = tfk.Sequential()\n    model.add(Conv2D(144, (3, 3), strides=(1, 1), padding=\"valid\", input_shape=(224, 224, 3)))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPool2D())\n    model.add(Conv2D(94, (3, 3), strides=(1, 1), padding=\"valid\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPool2D())\n    model.add(Conv2D(144, (3, 3), strides=(1, 1), padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPool2D())\n    model.add(Conv2D(94, (3, 3), strides=(1, 1), padding=\"valid\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPool2D())\n    model.add(Conv2D(42, (3, 3), strides=(1, 1), padding=\"valid\"))\n    model.add(BatchNormalization())\n    model.add(Activation(\"relu\"))\n    model.add(MaxPool2D())\n    model.add(GlobalAveragePooling2D())\n    model.add(Flatten())\n    model.add(Dropout(0.21))\n    model.add(Dense(256, activation=\"tanh\", kernel_regularizer=tf.keras.regularizers.l2()))\n    model.add(Dropout(0.21))\n    model.add(Dense(256, activation=\"tanh\", kernel_regularizer=tf.keras.regularizers.l2()))\n    model.add(Dropout(0.21))\n    model.add(Dense(num_classes, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"CABYt5oPVoF1","trusted":false},"cell_type":"code","source":"# Pretrained model, best with DenseNet, \n# Xception, NASNet, ResNet, EfficientNet, all were failed although the Dense layers were the same\ndef create_pretrained_model():\n    model = tfk.Sequential()\n    pretrained_net = tfk.applications.DenseNet201(\n        include_top=False,\n        input_shape=(224, 224, 3),\n        pooling=\"avg\"\n    )\n    model.add(pretrained_net)\n    model.add(Dropout(0.42))\n    model.add(Dense(256, activation=\"tanh\", kernel_regularizer=tf.keras.regularizers.l2()))\n    model.add(Dropout(0.21))\n    model.add(Dense(128, activation=\"tanh\"))\n    model.add(Dropout(0.21))\n    model.add(Dense(num_classes, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"WwWNxAxpCMTl","trusted":false},"cell_type":"code","source":"# Callbacks when val_acc reaches 99%, but best was 96% unfortunately\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if logs.get(\"val_accuracy\") is not None:\n            if(logs.get('val_accuracy') > 0.99):\n                print(\"\\nReached 99% val_accuracy so cancelling training!\")\n                self.model.stop_training = True\nmcb = myCallback()","execution_count":null,"outputs":[]},{"metadata":{"id":"m6JONgdqgPpO","outputId":"cef55481-0c2d-4a08-9e90-98b47d39cedb","trusted":false},"cell_type":"code","source":"epochs = 150\nuse_efficientnet = True\nif not use_efficientnet:\n    model = create_model()\nelse:\n    model = create_pretrained_model()\n\n\n# Learning rate decay of pretrained model, taken from the notebook TPUs in Colab\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005*8\nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n\ndef lrfn(epoch):\n    if epoch < rampup_epochs:\n        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n    elif epoch < rampup_epochs + sustain_epochs:\n        return max_lr\n    else:\n        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n\nmcp = tf.keras.callbacks.ModelCheckpoint(\"my_model.h5\", monitor=\"val_accuracy\",\n                        save_best_only=True, save_weights_only=True) # Save weights of best model based on val_acc\nval_acc_earlyStop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n                                                    patience = epochs//15, restore_best_weights = True) # If val_acc does not improve\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n\nmodel.fit(train_gen, validation_data=valid_gen, epochs=epochs, callbacks=[mcp, mcb, val_acc_earlyStop, lr_callback])","execution_count":null,"outputs":[]},{"metadata":{"id":"CdaH9lUUCMTy","trusted":false},"cell_type":"code","source":"model = create_pretrained_model()\nmodel.build(input_shape=(1,224,224,3))\nmodel.load_weights(\"my_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Hrgc1T-LCMT1","trusted":false},"cell_type":"code","source":"test_data_gen = ImageDataGenerator(rescale=1.0/255)\ntest_generator = test_data_gen.flow_from_directory(\"../input/vietai-c6-assignment3-extracted-dataset/test\",class_mode=None, target_size=(size, size), shuffle=False)\ntest_generator.reset()\npred = model.predict(test_generator)\n\n# Get best prediction\npred_labels = np.argmax(pred, axis=1)\nsub_df['label'] = pred_labels\nsub_df.head(20)","execution_count":null,"outputs":[]},{"metadata":{"id":"riWkqzkACMT7","trusted":false},"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}